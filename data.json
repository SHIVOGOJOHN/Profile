[
  {
    "id": "ml_audit",
    "title": "ML Audit Library",
    "description": "Solves Data Lineage Blindness by tracking granular preprocessing steps.",
    "tech_stack": [
      "Python",
      "Pandas",
      "Scikit-Learn"
    ],
    "details": "ml-audit is a lightweight Python library designed to bring transparency and reproducibility to data preprocessing. Unlike standard experiment trackers that treat preprocessing as a black box, this library records every granular transformation applied to your pandas DataFrame. Features include Full Audit Trail, Reproducibility checks, and Visualization of the preprocessing pipeline."
  },
  {
    "id": "creditworthiness",
    "title": "Project Creditworthiness",
    "description": "A production grade credit scoring model using XGBoost and adaptive SHAP explanations.",
    "tech_stack": [
      "Python",
      "XGBoost",
      "Flask"
    ],
    "details": "This project focuses on developing a robust and interpretable credit scoring model. The primary goal is to assess the creditworthiness of loan applicants with high accuracy while providing clear, human-understandable explanations for each decision. We utilize the XGBoost algorithm, known for its performance in competitive machine learning, and integrate it with SHAP (SHapley Additive exPlanations) to ensure model transparency. The entire solution is deployed as a microservice using Flask, making it easy to integrate with existing banking and financial systems. The model is trained on a diverse dataset of historical loan data, and extensive feature engineering is performed to capture complex relationships between applicant attributes and default risk."
  },
  {
    "id": "carbon_capture",
    "title": "Project Carbon Capture",
    "description": "AI-driven optimization for carbon capture technologies.",
    "tech_stack": [
      "Python",
      "PyTorch",
      "Gekko"
    ],
    "details": "This project aims to accelerate the development and efficiency of carbon capture and storage (CCS) technologies using artificial intelligence. We are developing a digital twin of a carbon capture process plant, which allows us to simulate and optimize operations in a virtual environment. The core of the project is a deep reinforcement learning agent that learns to control the capture process in real-time to maximize CO2 absorption while minimizing energy consumption. The Gekko optimization suite is used for dynamic optimization and control of the complex chemical processes involved. The ultimate goal is to create a scalable and cost-effective AI solution that can be deployed in real-world carbon capture facilities to help combat climate change."
  },
  {
    "id": "supply_chain",
    "title": "Project Supply Chain",
    "description": "Optimization of supply chain logistics using AI.",
    "tech_stack": [
      "Python",
      "TensorFlow",
      "OR-Tools"
    ],
    "details": "This project tackles the complex challenge of supply chain optimization through the application of AI and operations research. We are building a system that provides real-time visibility and predictive insights into supply chain operations. The system uses a combination of machine learning models to forecast demand, predict potential disruptions (e.g., shipping delays, supplier issues), and recommend optimal inventory levels. Google's OR-Tools is used to solve large-scale routing and scheduling problems, ensuring that goods are transported efficiently. The project's architecture is based on a microservices approach, allowing for scalability and easy integration with existing enterprise resource planning (ERP) systems. The result is a more resilient, efficient, and sustainable supply chain."
  },
  {
    "id": "biodiversity_ai",
    "title": "Project Biodiversity AI",
    "description": "AI for monitoring and preserving biodiversity.",
    "tech_stack": [
      "Python",
      "PyTorch",
      "OpenCV"
    ],
    "details": "This project leverages AI to automate the process of monitoring wildlife populations and assessing biodiversity from camera trap images and acoustic data. We are developing and training deep learning models, specifically convolutional neural networks (CNNs) for image analysis and recurrent neural networks (RNNs) for audio analysis, to automatically identify and classify different species. This automates a task that is traditionally manual and time-consuming for ecologists. The system can process vast amounts of data to provide near real-time insights into species distribution, population dynamics, and habitat health. This data is crucial for conservation efforts, helping to protect endangered species and preserve natural ecosystems."
  },
  {
    "id": "paper_adversarial_cybersecurity",
    "title": "Research on Adversarial ML in Cybersecurity",
    "description": "Exploring the use of adversarial machine learning for enhancing cybersecurity.",
    "tech_stack": [
      "Python",
      "TensorFlow",
      "CleverHans"
    ],
    "details": "This research investigates the dual role of adversarial machine learning in cybersecurity. On one hand, we explore how attackers can exploit vulnerabilities in machine learning models used for security applications (e.g., malware detection, intrusion detection) through adversarial examples. We use libraries like CleverHans to craft these attacks. On the other hand, we research how to build more robust and resilient security models by using adversarial training. This involves training models on a dataset augmented with adversarial examples, effectively teaching them to resist such attacks. The paper presents a novel framework for a continuous cycle of adversarial attack and defense, leading to progressively stronger security systems."
  },
  {
    "id": "paper_low_resource_med",
    "title": "Research on Low-Resource Medical NLP",
    "description": "Natural Language Processing for medical applications in low-resource settings.",
    "tech_stack": [
      "Python",
      "PyTorch",
      "Hugging Face Transformers"
    ],
    "details": "This research focuses on the challenge of applying advanced Natural Language Processing (NLP) techniques to medical domains where labeled data is scarce. We explore transfer learning and data augmentation techniques to adapt large pre-trained language models (like those from the Hugging Face Transformers library) to specific medical tasks, such as clinical entity recognition and patient outcome prediction, using only a small amount of annotated data. The paper proposes a new self-training methodology that leverages a model's own predictions on unlabeled data to improve its performance, demonstrating its effectiveness on several low-resource medical datasets. The goal is to make cutting-edge NLP accessible for a wider range of medical applications, particularly in specialized or under-resourced areas."
  },
  {
    "id": "paper_quantum_gnn",
    "title": "Research on Quantum Graph Neural Networks",
    "description": "Investigating the intersection of quantum computing and graph neural networks.",
    "tech_stack": [
      "Python",
      "PennyLane",
      "PyTorch Geometric"
    ],
    "details": "This research explores the potential of quantum computing to enhance the capabilities of Graph Neural Networks (GNNs). We are developing hybrid quantum-classical GNN models where the feature transformation and message passing steps are implemented as parameterized quantum circuits. Using libraries like PennyLane for quantum machine learning and PyTorch Geometric for GNNs, we investigate whether these quantum variants can learn more expressive representations of graph-structured data compared to their classical counterparts. The paper provides a theoretical analysis of the expressive power of quantum GNNs and presents experimental results on benchmark graph classification tasks, showing promising (though early) results for certain classes of problems."
  },
  {
    "id": "paper_defi_risk",
    "title": "Research on DeFi Risk Management",
    "description": "Agent-based modeling for risk management in decentralized finance (DeFi).",
    "tech_stack": [
      "Python",
      "Mesa",
      "CadCAD"
    ],
    "details": "This research uses agent-based modeling (ABM) to simulate and analyze systemic risks in the decentralized finance (DeFi) ecosystem. We create a simulated environment populated by autonomous agents representing different DeFi participants (e.g., liquidity providers, traders, arbitrageurs). Using frameworks like Mesa and CadCAD, we model their behaviors and interactions within various DeFi protocols. The simulation allows us to stress-test protocols under extreme market conditions and identify potential cascading failures, flash loan exploits, and other vulnerabilities that are difficult to predict with traditional analytical models. The paper presents insights into the complex dynamics of DeFi and proposes new risk management strategies for building a more stable and secure decentralized financial system."
  },
  {
    "id": "blockchain_ai",
    "title": "Federated Learning with Blockchain & IPFS",
    "description": "A decentralized, auditable, and transparent AI training ecosystem.",
    "tech_stack": [
      "TensorFlow",
      "Scikit-learn",
      "Flower",
      "FastAPI",
      "Flask",
      "Docker",
      "Ethereum",
      "IPFS",
      "Streamlit"
    ],
    "details": "This project integrates Federated Learning (FL) with Blockchain and IPFS to create a decentralized, auditable, and transparent AI training ecosystem. The system ensures that local data from different clients (e.g., stores, hospitals, IoT devices) remains private, while model updates are aggregated securely on a central node. Each aggregation round is recorded on the Ethereum blockchain for transparency, and the aggregated global model is stored on IPFS for decentralized access."
  },
  {
    "id": "paper_fair_explainable_credit",
    "title": "Fair and Explainable Credit-Scoring under Concept Drift",
    "description": "Adaptive Explanation Frameworks for Evolving Populations",
    "tech_stack": [
      "Fairness Auditing",
      "Explainability Stability",
      "Drift Detection",
      "Adaptive SHAP"
    ],
    "details": "This research explores how AI credit-scoring models can stay fair, transparent, and reliable over time even as customer behavior, economics, or demographics change. It introduces an adaptive explanation framework that monitors how model decisions evolve, detects data or concept drift, and ensures that explanations remain stable and fair across groups like gender, income, or region. Using metrics for drift detection, fairness auditing, and explainability stability (such as cosine similarity, Kendall tau, and Jaccard overlap), the study builds a foundation for ethical and trustworthy AI in financial decision-making."
  }
]